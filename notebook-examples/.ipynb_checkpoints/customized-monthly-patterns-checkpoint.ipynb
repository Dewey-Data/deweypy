{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7896032a-533a-4af7-ba0d-6d5445da678f",
   "metadata": {},
   "source": [
    "# Customized Monthly Patterns Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10cfccc-c0cf-45bb-911f-8e9edadeb223",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this example, we will:\n",
    "1. Navigate to the Foot Traffic / Monthly Patterns Dataset on Dewey.\n",
    "2. Select \"Get Data\".\n",
    "3. Review and accept license terms.\n",
    "4. Choose to \"Customize Data\" from Foot Traffic / Monthly Patterns instead of downloading all of it.\n",
    "5. Finish customizing the data, and continue on to downloading that customized data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce52a6-7584-4a47-8fc9-57e651fda66a",
   "metadata": {},
   "source": [
    "### 1. Navigate to the Foot Traffic / Monthly Patterns Dataset on Dewey.\n",
    "\n",
    "Go to [The dataset page](https://app.deweydata.io/data/advan/monthly-patterns-foot-traffic-container/monthly-patterns-foot-traffic)\n",
    "\n",
    "(If you can't access or the link is invalid, search \"Foot Traffic\" on the site and you should be able to find it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271e984-631f-4547-8565-87df363f6b75",
   "metadata": {},
   "source": [
    "### 2. Click Get Data\n",
    "\n",
    "Click \"Get Data\", currently in the upper right (although may vary depending on screen size)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5da1c-1136-4cc2-9292-0333251b66f6",
   "metadata": {},
   "source": [
    "### 3. Review and Accept License Terms\n",
    "\n",
    "You'll see data-provider specific license terms. Review and accept those if you'd like to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d7b14-addb-4223-bdd3-8d3708726dcb",
   "metadata": {},
   "source": [
    "### 4. Customize Data\n",
    "\n",
    "Use the filters on the left to select:\n",
    "1. The columns you care about. For this example I'll select all of them, but, you should only include the columns you care about.\n",
    "2. The filters that are relevant to your research. For this example, I'm going to select the US states of AK (Alaska) and WY (Wyoming) as the \"Region\"s that I care about.\n",
    "\n",
    "Once you've narrowed down to the columns and filters you want, click \"Generate Data\" to refresh the preview. Take a quick glance at the preview and make sure it looks like the data you'd want.\n",
    "\n",
    "For mine, I see the data I want, and I see:\n",
    "* Row Count Reduction Progress &nbsp;&nbsp;&nbsp; 4,385,129 rows &nbsp;&nbsp;&nbsp; 0.42%\n",
    "* Target: 206,656,143 (reduction to 20% recommended)\n",
    "\n",
    "Once you're ready, click \"Get Data\".\n",
    "\n",
    "It may take a few minutes for the data to be prepared for download. You'll get an email when it's done/ready. For this example, I got an email with the subject \"Customized Dataset Ready for Download\" and I followed the instructions in the email to open my customized dataset. I clicked \"Get Data\" on the customized dataset in \"My Datasets\" and see:\n",
    "* Rows: 4,385,129\n",
    "* Columns: 53\n",
    "* Size: 3.12 GB\n",
    "* Options:\n",
    "  * This file is too large to download as a CSV &nbsp;&nbsp;&nbsp; Download limits: 2GB\n",
    "  * Bulk API &nbsp;&nbsp;&nbsp; >\n",
    "\n",
    "Since it's too large to download the CSV directly (and we want to showcase Python Client + API usage here anyway), we'll click on \"Bulk API\" to get to the next screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea85e7-fe6e-4ba3-ad94-cc08975da562",
   "metadata": {},
   "source": [
    "### 5. Copy API URL and API Key\n",
    "\n",
    "You'll see an API URL. Copy and save that for reference.\n",
    "\n",
    "If this is your first time downloading data, you'll see your API Key as well. Copy and save that for reference. Alternatively, you can issue a new key on that page if you need a new one (will invalidate your old one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c6697-e0b4-418f-a927-1f056258b829",
   "metadata": {},
   "source": [
    "### 6. Download UV\n",
    "\n",
    "If you don't already have `uv` installed, install it: https://docs.astral.sh/uv/getting-started/installation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ceb729-cbd4-4983-82ab-ca4bccf3f01a",
   "metadata": {},
   "source": [
    "### 7. Download the Dataset\n",
    "\n",
    "Suppose, in this example, `api_url` will probably be something like `\"https://api.deweydata.io/api/v1/external/data/cdst_INSERT_YOUR_PART_HERE\"` (you get this from the Dewey Web App).\n",
    "\n",
    "You'll need to paste whatever you get properly below in order for these examples to work, and then comment out the first `api_url = ` (or remove the line), and uncomment the bottom `api_url = ` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7541e8e1-c33f-41b7-8580-456511fb3d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://api.deweydata.io/api/v1/external/data/cdst_69nriiuzp96tjqie\"\n",
    "# TODO: Paste your appropriate API URL, uncomment the below, and comment out or delete the above.\n",
    "# api_url = \"https://api.deweydata.io/api/v1/external/data/cdst_INSERT_YOUR_PART_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77fba1-ef74-4300-90c9-365f411f5b99",
   "metadata": {},
   "source": [
    "#### 7.1 - Download the Dataset - Shell Method\n",
    "\n",
    "The easiest way to download the dataset is to use your preferred programming shell (I.E. iTerm2, Bash, Windows Terminal, Etc.).\n",
    "\n",
    "You should:\n",
    "* Navigate to the directory you want to analyze this data at (I.E. `cd your/directory/you/want`).\n",
    "* Make sure you have your API Key accessible from above.\n",
    "\n",
    "Then, from your shell/terminal, paste and edit this command as necessary:\n",
    "\n",
    "```\n",
    "# TODO (Dewey Team): Change to proper `uv run` or `uvx run` command \n",
    "python -m deweypy download {api_url}\n",
    "```\n",
    "\n",
    "You should replace `{api_url}` with the `api_url` value above, and `api_key` with your API Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8735a45-e14c-435e-920b-90fa92da4f52",
   "metadata": {},
   "source": [
    "#### 7.2 - Download the Dataset - From Jupyter Notebook (Here) Method\n",
    "\n",
    "For now, this is fully documented or the implementation finalized. However, you may choose to instantiate an instance of `DatasetDownloader` and then call `download(...)` on it. For now, assume/expect this is subject to change as this is currently an internal class and method, but if you really want or need to do this all within the Jupyter notebook, this should work. The next shell shows how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2342839-176b-42b4-b1f8-c526e3c20ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Overall</span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">0.0%</span> • <span style=\"color: #008000; text-decoration-color: #008000\">0.0/3.4 GB</span> • <span style=\"color: #800000; text-decoration-color: #800000\">?</span> • <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span> • <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mOverall\u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0/3.4 GB\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:--\u001b[0m • \u001b[33m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Download Complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mDownload Complete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Files processed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Files processed: \u001b[1;36m79\u001b[0m/\u001b[1;36m79\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Successful downloads: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Successful downloads: \u001b[1;36m79\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Failed downloads: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Failed downloads: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Bytes downloaded: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">353</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">488</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">526</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Bytes downloaded: \u001b[1;36m3\u001b[0m,\u001b[1;36m353\u001b[0m,\u001b[1;36m488\u001b[0m,\u001b[1;36m526\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Bytes remaining: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Bytes remaining: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Data is downloaded!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Data is downloaded!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you really want to do this, change this value to `True`.\n",
    "DO_JUPYTER_DIRECT_DOWNLOAD: bool = False\n",
    "\n",
    "if DO_JUPYTER_DIRECT_DOWNLOAD:\n",
    "    import os\n",
    "    \n",
    "    from pathlib import Path\n",
    "    \n",
    "    from deweypy.auth import set_api_key\n",
    "    from deweypy.downloads import DatasetDownloader, set_download_directory\n",
    "\n",
    "    set_api_key(\"TODO-PUT-YOUR-API-KEY-HERE\")\n",
    "    # TODO: If you want, change the download directory.\n",
    "    download_directory = Path(f\".{os.sep}dewey-downloads\")\n",
    "    if not download_directory.exists():\n",
    "        print(f\"Creating download directory {download_directory}...\")\n",
    "        download_directory.mkdir(parents=True)\n",
    "    set_download_directory(download_directory)\n",
    "\n",
    "    # TODO (Dewey Team): Replace this with `api_url` without splitting\n",
    "    # or whatever should be the main way to do this once it's all ready.\n",
    "    downloader = DatasetDownloader(api_url.rsplit(\"/\", 1)[-1])\n",
    "    downloader.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22748b2b-6b4e-44ab-8912-cd0a58f83e5c",
   "metadata": {},
   "source": [
    "### 8 - Interlude\n",
    "\n",
    "So, you've got the dataset downloaded. Now what?\n",
    "\n",
    "First of all, the dataset is now stored, as a collection of files (usually gzipped CSVs or parquet files) in the folder that the dataset got downloaded to.\n",
    "\n",
    "So, *you can take it from here if you want*. You've successfully downloaded the data.\n",
    "\n",
    "If you'd like to see some examples of how to load, transform, and work with the data in general, feel free to proceed with this notebook. But, you're free to take that data folder and process it however you see fit from this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5183f-fbc0-4c7c-909c-3539fc650adf",
   "metadata": {},
   "source": [
    "### 9 - Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa7c6b2-96bf-4c8d-a11a-5e262294cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful imports\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ce85ca0-9153-45b3-9795-4a0f6fab653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dewey Downloads Directory: dewey-downloads\n",
      "This Dataset Directory: dewey-downloads/customized-monthly-patterns-example\n"
     ]
    }
   ],
   "source": [
    "# These should all be properly defined based on the results above.\n",
    "dewey_downloads_directory = Path(f\".{os.sep}dewey-downloads\")\n",
    "assert dewey_downloads_directory.exists(), \"Pre-condition\"\n",
    "# TODO: Replace with whatever your final sub-folder name ended up being.\n",
    "this_dataset_sub_folder_name = \"customized-monthly-patterns-example\"\n",
    "this_dataset_directory = dewey_downloads_directory / this_dataset_sub_folder_name\n",
    "assert this_dataset_directory.exists(), \"Pre-condition\"\n",
    "\n",
    "print(f\"Dewey Downloads Directory: {dewey_downloads_directory}\")\n",
    "print(f\"This Dataset Directory: {this_dataset_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9774f14-de93-4756-8bec-d0966be66835",
   "metadata": {},
   "source": [
    "### Example - Polars CSV Scan and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90c7f5f1-a376-4f4f-9aee-4b675ce926cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you don't have `polars` installed, you should install it into your Python environment\n",
    "# (I.E. with `pip`, `uv` or some other tool).\n",
    "import polars as pl\n",
    "\n",
    "# NOTE: You can set this to `True` if your machine doesn't have a lot of memory available.\n",
    "polars_low_memory = False\n",
    "\n",
    "# NOTE: Depending on the size of your dataset, this might take several seconds to\n",
    "# several minutes or even longer in some cases.\n",
    "df = pl.scan_csv(this_dataset_directory, low_memory=polars_low_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0dc33c-3469-49c5-9bb6-4655457a1f31",
   "metadata": {},
   "source": [
    "### Example (Continued) - Polars CSV Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f3eeecb-40fe-4ca6-bdc6-58f6bdc52181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4,385,129\n"
     ]
    }
   ],
   "source": [
    "# Very basic: Get a total row count directly from the `df`.\n",
    "total_rows = df.select(pl.len()).collect()\n",
    "print(f\"Total rows: {total_rows.item():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ff77c4f-1677-4312-b977-43bba0f6bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column schema:\n",
      "  BRANDS: String\n",
      "  BUCKETED_DWELL_TIMES: String\n",
      "  CATEGORY_TAGS: String\n",
      "  CITY: String\n",
      "  CLOSED_ON: String\n",
      "  DATE_RANGE_END: String\n",
      "  DATE_RANGE_START: String\n",
      "  DEVICE_TYPE: String\n",
      "  DISTANCE_FROM_HOME: String\n",
      "  ENCLOSED: String\n",
      "  GEOMETRY_TYPE: String\n",
      "  INCLUDES_PARKING_LOT: String\n",
      "  ISO_COUNTRY_CODE: String\n",
      "  IS_SYNTHETIC: String\n",
      "  LATITUDE: Float64\n",
      "  LOCATION_NAME: String\n",
      "  LONGITUDE: Float64\n",
      "  MEDIAN_DWELL: String\n",
      "  NAICS_CODE: Int64\n",
      "  NORMALIZED_VISITS_BY_REGION_NAICS_VISITORS: String\n",
      "  NORMALIZED_VISITS_BY_REGION_NAICS_VISITS: String\n",
      "  NORMALIZED_VISITS_BY_STATE_SCALING: String\n",
      "  NORMALIZED_VISITS_BY_TOTAL_VISITORS: String\n",
      "  NORMALIZED_VISITS_BY_TOTAL_VISITS: String\n",
      "  OPENED_ON: String\n",
      "  OPEN_HOURS: String\n",
      "  PARENT_PLACEKEY: String\n",
      "  PHONE_NUMBER: String\n",
      "  PLACEKEY: String\n",
      "  POI_CBG: String\n",
      "  POLYGON_CLASS: String\n",
      "  POLYGON_WKT: String\n",
      "  POPULARITY_BY_DAY: String\n",
      "  POPULARITY_BY_HOUR: String\n",
      "  POSTAL_CODE: String\n",
      "  RAW_VISITOR_COUNTS: String\n",
      "  RAW_VISIT_COUNTS: String\n",
      "  REGION: String\n",
      "  RELATED_SAME_DAY_BRAND: String\n",
      "  RELATED_SAME_MONTH_BRAND: String\n",
      "  SAFEGRAPH_BRAND_IDS: String\n",
      "  STORE_ID: String\n",
      "  STREET_ADDRESS: String\n",
      "  SUB_CATEGORY: String\n",
      "  TOP_CATEGORY: String\n",
      "  TRACKING_CLOSED_SINCE: String\n",
      "  VISITOR_COUNTRY_OF_ORIGIN: String\n",
      "  VISITOR_DAYTIME_CBGS: String\n",
      "  VISITOR_HOME_AGGREGATION: String\n",
      "  VISITOR_HOME_CBGS: String\n",
      "  VISITS_BY_DAY: String\n",
      "  WEBSITES: String\n",
      "  WKT_AREA_SQ_METERS: String\n"
     ]
    }
   ],
   "source": [
    "# Get the Polars-inferred schema.\n",
    "schema = df.collect_schema()\n",
    "print(\"Column schema:\")\n",
    "for name, dtype in schema.items():\n",
    "    print(f\"  {name}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae934c98-3db7-4ec9-9aa5-ac1ac1286c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK count: 2,340,622\n",
      "WY count: 2,044,507\n"
     ]
    }
   ],
   "source": [
    "# Region counts (specific to this dataset AK/WY example. Change this if you\n",
    "# changed the dataset or filter).\n",
    "region_counts = df.group_by(\"REGION\").agg(pl.len().alias(\"count\")).collect()\n",
    "ak_count = region_counts.filter(pl.col(\"REGION\") == \"AK\").select(\"count\").item()\n",
    "wy_count = region_counts.filter(pl.col(\"REGION\") == \"WY\").select(\"count\").item()\n",
    "print(f\"AK count: {ak_count:,}\")\n",
    "print(f\"WY count: {wy_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2ed85a7-4c4d-47de-b728-cb520187f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Locations:\n",
      "shape: (10, 4)\n",
      "┌─────────────────────┬───────────────────────────────┬───────────┬────────────────┐\n",
      "│ PLACEKEY            ┆ LOCATION_NAME                 ┆ CITY      ┆ total_visitors │\n",
      "│ ---                 ┆ ---                           ┆ ---       ┆ ---            │\n",
      "│ str                 ┆ str                           ┆ str       ┆ i64            │\n",
      "╞═════════════════════╪═══════════════════════════════╪═══════════╪════════════════╡\n",
      "│ zzw-227@3bt-byj-nt9 ┆ Dimond Center                 ┆ Anchorage ┆ 920492         │\n",
      "│ zzw-223@3bt-byj-qzz ┆ Diamond Center Farmers Market ┆ Anchorage ┆ 920492         │\n",
      "│ zzy-224@3bt-bz3-2hq ┆ InMotion Entertainment        ┆ Anchorage ┆ 914796         │\n",
      "│ 222-222@3bt-c2m-kxq ┆ Merrill Field                 ┆ Anchorage ┆ 887177         │\n",
      "│ 223-224@3bt-bz2-yd9 ┆ Hudson News                   ┆ Anchorage ┆ 682632         │\n",
      "│ zzw-225@3bt-bz3-2p9 ┆ AK&CO Gourmet Market          ┆ Anchorage ┆ 682632         │\n",
      "│ zzw-22g@3bt-bz3-2p9 ┆ Alaska Mercantile             ┆ Anchorage ┆ 682632         │\n",
      "│ 223-225@3bt-bz2-yd9 ┆ Yakutia Airlines              ┆ Anchorage ┆ 682632         │\n",
      "│ 223-22c@3bt-bz2-yd9 ┆ Huntleigh USA Corporation     ┆ Anchorage ┆ 682632         │\n",
      "│ 223-22b@3bt-bz2-yd9 ┆ Republic Parking              ┆ Anchorage ┆ 682632         │\n",
      "└─────────────────────┴───────────────────────────────┴───────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Some more advanced analytics examples:\n",
    "\n",
    "# Convert string columns to more specific types where needed.\n",
    "df = df.with_columns([\n",
    "    pl.col(\"RAW_VISITOR_COUNTS\").cast(pl.Int64, strict=False).alias(\"visitors\"),\n",
    "    pl.col(\"RAW_VISIT_COUNTS\").cast(pl.Int64, strict=False).alias(\"visits\"), \n",
    "    pl.col(\"MEDIAN_DWELL\").cast(pl.Float64, strict=False).alias(\"dwell_minutes\"),\n",
    "    pl.col(\"WKT_AREA_SQ_METERS\").cast(pl.Float64, strict=False).alias(\"area_sq_m\"),\n",
    "    pl.col(\"DATE_RANGE_START\").str.to_datetime(\"%Y-%m-%d %H:%M:%S%.f\").dt.date().alias(\"start_date\")\n",
    "])\n",
    "\n",
    "# Top N busiest locations by visitor count.\n",
    "top_locations = (df.group_by([\"PLACEKEY\", \"LOCATION_NAME\", \"CITY\"])\n",
    "                 .agg(pl.col(\"visitors\").sum().alias(\"total_visitors\"))\n",
    "                 .sort(\"total_visitors\", descending=True)\n",
    "                 .head(10)\n",
    "                 .collect())\n",
    "\n",
    "print(\"Top Locations:\")\n",
    "print(top_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07ede47e-5475-40eb-a0e1-2293a722d85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Visitor Trends by State:\n",
      "shape: (24, 3)\n",
      "┌───────┬────────┬──────────────────┐\n",
      "│ month ┆ REGION ┆ monthly_visitors │\n",
      "│ ---   ┆ ---    ┆ ---              │\n",
      "│ i8    ┆ str    ┆ i64              │\n",
      "╞═══════╪════════╪══════════════════╡\n",
      "│ 1     ┆ AK     ┆ 26261111         │\n",
      "│ 2     ┆ AK     ┆ 24702853         │\n",
      "│ 3     ┆ AK     ┆ 27042809         │\n",
      "│ 4     ┆ AK     ┆ 25209806         │\n",
      "│ 5     ┆ AK     ┆ 34008552         │\n",
      "│ …     ┆ …      ┆ …                │\n",
      "│ 8     ┆ WY     ┆ 27560985         │\n",
      "│ 9     ┆ WY     ┆ 22987054         │\n",
      "│ 10    ┆ WY     ┆ 19281135         │\n",
      "│ 11    ┆ WY     ┆ 15399589         │\n",
      "│ 12    ┆ WY     ┆ 15845013         │\n",
      "└───────┴────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Monthly visitor trends by state.\n",
    "monthly_trends = (df.group_by([pl.col(\"start_date\").dt.month().alias(\"month\"), \"REGION\"])\n",
    "                  .agg(pl.col(\"visitors\").sum().alias(\"monthly_visitors\"))\n",
    "                  .sort([\"REGION\", \"month\"])\n",
    "                  .collect())\n",
    "\n",
    "print(\"Monthly Visitor Trends by State:\")\n",
    "print(monthly_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255cb249-e2f8-4af3-9ac7-156814fd6b6b",
   "metadata": {},
   "source": [
    "### Example - Faster Polars Processing and Loading/Restoring\n",
    "\n",
    "Let's say some of all of the following is true:\n",
    "* You want to run a lot of different types of queries on this data.\n",
    "* You expect to come back to this notebook multiple times, maybe over the course of multiple days or weeks.\n",
    "* You'd like these queries to run as fast as possible, especially after coming back.\n",
    "\n",
    "The `scan_csv` approach above works, but:\n",
    "* Polars has to directly read dozens of CSV files (or many more if you have a bigger dataset), which is going to take some time.\n",
    "* The CSVs are gzipped, so Polars has to uncompress them (presumably) on the fly.\n",
    "* CSVs in general are one of the least optimized file types for doing data analysis.\n",
    "* AND, every time you restart the Jupyter Notebook Kernel, it has to redo all of this work.\n",
    "\n",
    "There are a number of ways you could optimize this. For example:\n",
    "* You could load all of the CSVs into a single dataframe in an eager (non-lazy) way.\n",
    "  * From there, subsequent queries (especially if you run a bunch of different ones that use the same intermediate results) should be faster than if you just used `scan_csv` at the beginning (but there are tradeoffs, see the Polars documentation links below).\n",
    "  * See some of these polars documentation references for more background and information:\n",
    "    * https://docs.pola.rs/user-guide/io/multiple/\n",
    "    * https://docs.pola.rs/user-guide/concepts/lazy-api/\n",
    "    * https://docs.pola.rs/user-guide/lazy/\n",
    "    * https://docs.pola.rs/api/python/stable/reference/api/polars.scan_csv.html\n",
    "    * https://docs.pola.rs/api/python/stable/reference/api/polars.read_csv.html\n",
    "* You could dump the data into a more optimized format for data analysis.\n",
    "  * Parquet is one such format. From the [polars docs](https://docs.pola.rs/user-guide/io/parquet/), \"Loading or writing [Parquet](https://parquet.apache.org/) files is lightning fast as the layout of data in a Polars DataFrame in memory mirrors the layout of a Parquet file on disk in many respects.\"\n",
    "* You could dump the data into a dedicated DB for analysis. That DB would then (presumably) store the data in an efficient format, and make it easy to query/load that data back into Polars quickly.\n",
    "  * For this, we'll use DuckDB as it [pairs nicely with Polars](https://duckdb.org/docs/stable/guides/python/polars.html).\n",
    " \n",
    "For these examples, we're going to show:\n",
    "* Dumping the data into one or more parquet files (and then loading it back in).\n",
    "  * We'll also benchmark using `scan_csv` with the gzipped CSVs vs using parquet files.\n",
    "* Dumping the data into a dedicated table in DuckDB (and then loading it back in).\n",
    "  * We'll also benchmark using `scan_csv` with the gzipped CSVs vs using DuckDB.\n",
    "\n",
    "The goal here is not to be exhaustive, but to give you a couple strategies you can use that will help, especially with bigger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66d1eb9a-fc68-4953-8462-bd1b336223d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partitioned files: dewey-downloads/customized-monthly-patterns-example/parquet\n"
     ]
    }
   ],
   "source": [
    "# Convert CSV to Parquet for faster querying and loading/restoring.\n",
    "\n",
    "# This examples presumes you've run the above relevant polars cells\n",
    "# (and have `df` defined as a polars dataframe. If you've imported and used\n",
    "# pandas instead, this cell may not work at all or may not work as expected).\n",
    "\n",
    "# Define the directory for the Parquet files.\n",
    "this_dataset_parquet_directory = this_dataset_directory / \"parquet\"\n",
    "if not this_dataset_parquet_directory.exists():\n",
    "    print(f\"Creating dataset parquet directory {this_dataset_parquet_directory}...\")\n",
    "    this_dataset_parquet_directory.mkdir(parents=True)\n",
    "\n",
    "# NOTE: If you'd like, you can filter the `df` before dumping it.\n",
    "# This can be a helpful strategy if you want to exclude data\n",
    "# with missing values relevant to your research or more granularly\n",
    "# filter the data than Dewey's web app allows for.\n",
    "#\n",
    "# This example filters out rows where \"RAW_VISITOR_COUNTS\"\n",
    "# is null or the empty string.\n",
    "#\n",
    "# Other Important NOTE: If you want to do this filtering,\n",
    "# make sure to replace `df` below with `filtered_df` in places\n",
    "# where you want the filtered data instead of the entire data\n",
    "# (I.E. do `df_for_partitioning = filtered_df....` instead of\n",
    "# `df_for_partitioning = df....`).\n",
    "# For this example for now, we'll include the entire dataset\n",
    "# so that we can have the same data for benchmark comparisons. \n",
    "filtered_df = df.filter(\n",
    "    (pl.col(\"RAW_VISITOR_COUNTS\").is_not_null()) & \n",
    "    (pl.col(\"RAW_VISITOR_COUNTS\") != \"\")\n",
    ")\n",
    "\n",
    "# NOTE: Using `partition_by` is completely up to you. You could\n",
    "# not include it (and not put `{part}` in the first argument).\n",
    "# Since these example queries analyze region and month, I'll\n",
    "# choose to partition the Parquet files that way as it will provide somewhat\n",
    "# better performance for `polars` later since it can utilize\n",
    "# predicate pushdown.\n",
    "df_for_partitioning = (\n",
    "    df\n",
    "    .with_columns(\n",
    "        pl.col(\"DATE_RANGE_START\")\n",
    "          .str.to_datetime(\"%Y-%m-%d %H:%M:%S%.f\", strict=True)\n",
    "          .dt.strftime(\"%Y-%m\")\n",
    "          .alias(\"year_month\")\n",
    "    )\n",
    ")\n",
    "partition_by = pl.PartitionByKey(\n",
    "    base_path=this_dataset_parquet_directory,\n",
    "    by=[\"REGION\", \"year_month\"],\n",
    ")\n",
    "df_for_partitioning.sink_parquet(\n",
    "    partition_by,\n",
    "    compression=\"zstd\",\n",
    "    maintain_order=False,\n",
    "    mkdir=True,\n",
    "    lazy=False,\n",
    ")\n",
    "print(f\"Saved partitioned files: {this_dataset_parquet_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75123472-d679-411c-800d-02a70f104197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File size comparison:\n",
      "Original CSV (gzipped): 3.12 GB\n",
      "Partitioned Parquet: 1.61 GB\n",
      "Compression ratio: 1.94x\n"
     ]
    }
   ],
   "source": [
    "# Now compare the total size of the gzipped CSVs to the Parquet files\n",
    "# (which also have compression).\n",
    "csv_size = sum(f.stat().st_size for f in Path(this_dataset_directory).glob('*.csv.gz'))\n",
    "partitioned_files = list(this_dataset_parquet_directory.glob('**/*.parquet'))\n",
    "parquet_size = sum(f.stat().st_size for f in partitioned_files)\n",
    "\n",
    "print(f\"\\nFile size comparison:\")\n",
    "print(f\"Original CSV (gzipped): {csv_size / (1024**3):.2f} GB\")\n",
    "print(f\"Partitioned Parquet: {parquet_size / (1024**3):.2f} GB\")\n",
    "print(f\"Compression ratio: {csv_size / parquet_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22d22eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "shape: (24, 3)\n",
      "┌───────┬────────┬──────────────────┐\n",
      "│ month ┆ REGION ┆ monthly_visitors │\n",
      "│ ---   ┆ ---    ┆ ---              │\n",
      "│ i8    ┆ str    ┆ i64              │\n",
      "╞═══════╪════════╪══════════════════╡\n",
      "│ 1     ┆ AK     ┆ 26261111         │\n",
      "│ 2     ┆ AK     ┆ 24702853         │\n",
      "│ 3     ┆ AK     ┆ 27042809         │\n",
      "│ 4     ┆ AK     ┆ 25209806         │\n",
      "│ 5     ┆ AK     ┆ 34008552         │\n",
      "│ …     ┆ …      ┆ …                │\n",
      "│ 8     ┆ WY     ┆ 27560985         │\n",
      "│ 9     ┆ WY     ┆ 22987054         │\n",
      "│ 10    ┆ WY     ┆ 19281135         │\n",
      "│ 11    ┆ WY     ┆ 15399589         │\n",
      "│ 12    ┆ WY     ┆ 15845013         │\n",
      "└───────┴────────┴──────────────────┘\n",
      "Parquet\n",
      "shape: (24, 3)\n",
      "┌───────┬────────┬──────────────────┐\n",
      "│ month ┆ REGION ┆ monthly_visitors │\n",
      "│ ---   ┆ ---    ┆ ---              │\n",
      "│ i8    ┆ str    ┆ i64              │\n",
      "╞═══════╪════════╪══════════════════╡\n",
      "│ 1     ┆ AK     ┆ 26261111         │\n",
      "│ 2     ┆ AK     ┆ 24702853         │\n",
      "│ 3     ┆ AK     ┆ 27042809         │\n",
      "│ 4     ┆ AK     ┆ 25209806         │\n",
      "│ 5     ┆ AK     ┆ 34008552         │\n",
      "│ …     ┆ …      ┆ …                │\n",
      "│ 8     ┆ WY     ┆ 27560985         │\n",
      "│ 9     ┆ WY     ┆ 22987054         │\n",
      "│ 10    ┆ WY     ┆ 19281135         │\n",
      "│ 11    ┆ WY     ┆ 15399589         │\n",
      "│ 12    ┆ WY     ┆ 15845013         │\n",
      "└───────┴────────┴──────────────────┘\n",
      "\n",
      "Performance comparison:\n",
      "CSV scan time: 6.62 seconds\n",
      "Parquet scan time: 0.16 seconds\n",
      "Speedup: 42.7x faster\n"
     ]
    }
   ],
   "source": [
    "# Now, demonstrate loading the dataframe from the partitioned\n",
    "# files, and compare querying from optimized/partitioned\n",
    "# Parquet files against querying from gzipped CSVs.\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "time_original_start = perf_counter()\n",
    "df_reset = pl.scan_csv(this_dataset_directory / '**/*.csv.gz', low_memory=False)\n",
    "df_reset = df_reset.with_columns([\n",
    "    pl.col(\"RAW_VISITOR_COUNTS\").cast(pl.Int64, strict=False).alias(\"visitors\"),\n",
    "    pl.col(\"RAW_VISIT_COUNTS\").cast(pl.Int64, strict=False).alias(\"visits\"), \n",
    "    pl.col(\"MEDIAN_DWELL\").cast(pl.Float64, strict=False).alias(\"dwell_minutes\"),\n",
    "    pl.col(\"WKT_AREA_SQ_METERS\").cast(pl.Float64, strict=False).alias(\"area_sq_m\"),\n",
    "    pl.col(\"DATE_RANGE_START\").str.to_datetime(\"%Y-%m-%d %H:%M:%S%.f\").dt.date().alias(\"start_date\")\n",
    "])\n",
    "monthly_trends_original = (df_reset.group_by([pl.col(\"start_date\").dt.month().alias(\"month\"), \"REGION\"])\n",
    "                  .agg(pl.col(\"visitors\").sum().alias(\"monthly_visitors\"))\n",
    "                  .sort([\"REGION\", \"month\"])\n",
    "                  .collect())\n",
    "print(\"Original\")\n",
    "print(monthly_trends_original)\n",
    "time_original_end = perf_counter()\n",
    "time_original_elapsed = time_original_end - time_original_start\n",
    "\n",
    "time_parquet_start = perf_counter()\n",
    "df_pq = pl.scan_parquet(this_dataset_parquet_directory / '**/*.parquet')\n",
    "monthly_trends_parquet = (df_pq.group_by([pl.col(\"start_date\").dt.month().alias(\"month\"), \"REGION\"])\n",
    "                  .agg(pl.col(\"visitors\").sum().alias(\"monthly_visitors\"))\n",
    "                  .sort([\"REGION\", \"month\"])\n",
    "                  .collect())\n",
    "print(\"Parquet\")\n",
    "print(monthly_trends_parquet)\n",
    "time_parquet_end = perf_counter()\n",
    "time_parquet_elapsed = time_parquet_end - time_parquet_start\n",
    "\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"CSV scan time: {time_original_elapsed:.2f} seconds\")\n",
    "print(f\"Parquet scan time: {time_parquet_elapsed:.2f} seconds\")\n",
    "print(f\"Speedup: {time_original_elapsed/time_parquet_elapsed:.1f}x faster\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0417b51",
   "metadata": {},
   "source": [
    "## Why Parquet is Faster\n",
    "\n",
    "**Parquet Advantages:**\n",
    "- **Columnar format**: Only reads columns you need\n",
    "- **Compression**: Typically 70-90% smaller than CSV\n",
    "- **Predicate pushdown**: Filters data at file level before loading\n",
    "- **Partitioning**: Split data by common query fields (like REGION)\n",
    "- **Schema preservation**: No type inference needed\n",
    "\n",
    "**Your Use Case Benefits:**\n",
    "- 4.4M rows will compress to ~500MB-1GB\n",
    "- Loading time: ~10-30x faster than CSV\n",
    "- Query time: ~5-10x faster, especially with partitioning\n",
    "- Memory usage: ~50% less than loading CSV\n",
    "\n",
    "**Partitioning Strategy:**\n",
    "- Partition by REGION (AK/WY) for state-specific queries\n",
    "- Consider partitioning by month if you query by time periods\n",
    "- Use `partition_by=['REGION', 'month']` for multi-level partitioning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
